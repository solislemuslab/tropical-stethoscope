{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"classification.ipynb","provenance":[],"collapsed_sections":["1CdE_YFPIMP3","w8CrxhnkVDN1","n0nBVUOeU8UZ","L_mIU5MhrOjE"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"S_IlT3rGnnrn","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1603494344666,"user_tz":300,"elapsed":5815,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}},"outputId":"1d140030-b7a9-4996-f11d-e4f8691336bc"},"source":["%tensorflow_version 1.x\n","import os\n","import h5py\n","import random\n","import datetime\n","import numpy as np\n","import pandas as pd\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","import keras\n","from keras.optimizers import Adam\n","from keras.applications import VGG19\n","from keras.utils import to_categorical, plot_model\n","from tensorflow.keras.callbacks import TensorBoard\n","from keras.models import Model, Sequential, load_model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.layers import Dense, Activation, Flatten, Dropout, Input, concatenate\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from datetime import datetime"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_7m2lg0UnxA8","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603494366937,"user_tz":300,"elapsed":17614,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}},"outputId":"ef1f6e7e-fdac-4e3c-a877-bbac70a0403f"},"source":["drive.mount('/content/drive')\n","os.chdir('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X_68PcQtuMuk"},"source":["# Read the dataset and preprocess"]},{"cell_type":"code","metadata":{"id":"gtDaAT-SuMRv"},"source":["# f =  h5py.File('My Drive/Stethoscope/13A_data.hdf5', \"r+\")\n","\n","# specs = np.array(f[\"specs\"]).astype(\"float32\")\n","# sonotypes = np.array(f[\"sonotypes\"])\n","# times = np.array(f[\"times\"]).astype(\"float32\")\n","# freqs = np.array(f[\"freqs\"]).astype(\"float32\")\n","\n","# f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1CdE_YFPIMP3"},"source":["## By groups"]},{"cell_type":"code","metadata":{"id":"l40ww_vcMsN2"},"source":["f =  h5py.File('My Drive/Stethoscope/data_group.hdf5', \"r+\")\n","\n","specs_h5 = np.array(f[\"specs\"]).astype(\"float32\")\n","sonotypes_h5 = np.array(f[\"groups\"])\n","times_h5 = np.array(f[\"times\"]).astype(\"float32\")\n","freqs_h5 = np.array(f[\"freqs\"]).astype(\"float32\")\n","\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FvAu65o_aaJI","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"5b3b8ebe-5183-4030-da96-f1d721e9fa8b"},"source":["# append x_times an x_freqs to be auxiliary_input\n","aux_input_h5 = np.append(times_h5,freqs_h5, axis = 1)\n","\n","\n","print(aux_input_h5.shape)\n","print(aux_input_h5[10])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(2881, 4)\n","[22293.    22321.469  7993.379  8626.394]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HmYSUydLn-pi","colab":{"base_uri":"https://localhost:8080/","height":108},"outputId":"c1ca769d-0e95-4a57-8d29-da983142d834"},"source":["# use only \"b\" and \"i\"\n","index_b = np.argwhere(sonotypes_h5 == b'b').flatten()\n","index_i = np.argwhere(sonotypes_h5 == b'i').flatten()\n","\n","print(\"number of data in group i: %i\" % len(index_i))\n","\n","# randomly choose index in b \n","# to ensure we have same number of type b and i\n","random.shuffle(index_b)\n","index_b_resized = index_b[:len(index_i)]\n","print(\"number of data in group b after resizing: %i\" % len(index_b_resized))\n","\n","# get the data of b and i\n","specs_b = specs_h5[index_b_resized]\n","specs_i = specs_h5[index_i]\n","aux_input_b = aux_input_h5[index_b_resized]\n","aux_input_i = aux_input_h5[index_i]\n","\n","# print(index_b_resized[1])\n","# print(times_b[1])\n","# print(times[index_b_resized[1]])\n","\n","# put them together\n","specs = np.append(specs_b, specs_i, axis = 0)\n","aux_input = np.append(aux_input_b, aux_input_i, axis= 0)\n","sonotypes = np.append(np.repeat(b'b', len(index_i)), np.repeat(b'i', len(index_i)))\n","\n","print(specs.shape)\n","print(aux_input.shape)\n","print(sonotypes)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["number of data in group i: 1021\n","number of data in group b after resizing: 1021\n","(2042, 224, 224, 3)\n","(2042, 4)\n","[b'b' b'b' b'b' ... b'i' b'i' b'i']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HqefOrdcITGp"},"source":["## By top k\n"]},{"cell_type":"code","metadata":{"id":"HZud_U8FIkYy"},"source":["f =  h5py.File('My Drive/Stethoscope/augment_data_0922.hdf5', \"r\")\n","\n","specs_h5 = np.array(f[\"specs\"]).astype(\"float32\")\n","sonotypes_h5 = np.array(f[\"sonotypes\"]).astype(\"float32\")\n","times_h5 = np.array(f[\"times\"]).astype(\"float32\")\n","freqs_h5 = np.array(f[\"freqs\"]).astype(\"float32\")\n","# groups_h5 = np.array(f[\"groups\"])\n","# selection_h5 = np.array(f[\"selections\"])\n","\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5eGypn-hOOhm","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1596377820966,"user_tz":300,"elapsed":339,"user":{"displayName":"YUREN SUN","photoUrl":"","userId":"11675060901325192603"}},"outputId":"89e1d7f2-ee95-40f9-9d2b-f1292757690b"},"source":["# append x_times an x_freqs to be auxiliary_input\n","aux_input_h5 = np.append(times_h5,freqs_h5, axis = 1)\n","\n","print(aux_input_h5.shape)\n","print(aux_input_h5[10])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(2999, 4)\n","[22293.834 22337.82   7993.379  8626.394]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bMooWTWOIxOi","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1596377822191,"user_tz":300,"elapsed":548,"user":{"displayName":"YUREN SUN","photoUrl":"","userId":"11675060901325192603"}},"outputId":"6928bc47-c620-4934-97f4-3ecd8e81b001"},"source":["# create the dictionary for sonotypes and groups\n","sono2group = dict(zip(sonotypes_h5,groups_h5))\n","\n","# get the data for top k sonotypes\n","s_unique, s_freq = np.unique(sonotypes_h5,return_counts=True)\n","s_freq_order = np.argsort(s_freq)[::-1]\n","s_freq_desc = s_freq[s_freq_order]\n","\n","print(len(s_unique))\n","print(s_unique[s_freq_order][:10])\n","print(s_freq_desc[:10])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["441\n","[ 52. 138.  25. 463. 236.   1. 167.  86. 175. 220.]\n","[175 119 114  91  74  69  60  57  49  47]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LMDMPTAdU9HQ"},"source":["##### use a balanced set of input\n"]},{"cell_type":"code","metadata":{"id":"hgGaHs0yoT-5","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1595031024143,"user_tz":300,"elapsed":612,"user":{"displayName":"YUREN SUN","photoUrl":"","userId":"11675060901325192603"}},"outputId":"4ddf2d1c-e056-4dec-d72e-051f94699ece"},"source":["numUsed =7\n","groupUsed = b'b' # use bird for now\n","typeUsed = []\n","min_num = 1000\n","\n","\n","# typeUsed=[52,138,463]\n","# min_num =91\n","index = 0\n","while len(typeUsed) < numUsed:\n","  cur_type = s_unique[s_freq_order][index]\n","  if sono2group[cur_type] == groupUsed:\n","    typeUsed.append(cur_type)\n","    if s_freq_desc[index] < min_num:\n","      min_num = s_freq_desc[index]\n","  index += 1\n","\n","print(typeUsed)\n","print(min_num)\n","# typeUsed = s_unique[s_freq_order][:numUsed]\n","# min_num = s_freq_desc[numUsed -1] # the number of every sonoytpes\n","\n","specs = []\n","aux_input = []\n","sonotypes = []\n","selections= []\n","\n","for i in range(len(typeUsed)):\n","  # get index of the current type of spec\n","  cur_index = np.argwhere(sonotypes_h5 == typeUsed[i]).flatten()\n","  # randomly choose index in b \n","  # to ensure we have same number of type b and i\n","  random.shuffle(cur_index)\n","  cur_index_resized = cur_index[:min_num]\n","  if len(specs):\n","    specs = np.append(specs, specs_h5[cur_index_resized], axis = 0)\n","  else:\n","    specs = specs_h5[cur_index_resized]\n","\n","  if len(aux_input):\n","    aux_input = np.append(aux_input, aux_input_h5[cur_index_resized], axis= 0)\n","  else:\n","    aux_input = aux_input_h5[cur_index_resized]\n","  \n","  if len(sonotypes):\n","    sonotypes = np.append(sonotypes, np.repeat(i, min_num))\n","  else:\n","    sonotypes = np.repeat(i, min_num)\n","\n","  if len(selections):\n","    selections = np.append(selections, selection_h5[cur_index_resized], axis= 0)\n","  else:\n","    selections = selection_h5[cur_index_resized]\n","\n","print(specs.shape)\n","print(aux_input.shape)\n","# print(selections)\n","# print(sonotypes)\n","specs_keep = np.copy(specs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[52, 138, 463]\n","91\n","(273, 224, 224, 3)\n","(273, 4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w8CrxhnkVDN1"},"source":["#### use a inbalanced set of input"]},{"cell_type":"code","metadata":{"id":"LIKQh6WRxGIU","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1594613837437,"user_tz":300,"elapsed":833,"user":{"displayName":"YUREN SUN","photoUrl":"","userId":"11675060901325192603"}},"outputId":"685495f2-a9de-484e-9568-d58d0db08123"},"source":["numUsed = 4\n","typeUsed = s_unique[s_freq_order][:numUsed]\n","# min_num = s_freq_desc[numUsed -1] # the number of every sonoytpes\n","\n","specs = []\n","aux_input = []\n","sonotypes = []\n","\n","for i in range(numUsed):\n","  # get index of the current type of spec\n","  cur_index = np.argwhere(sonotypes_h5 == typeUsed[i]).flatten()\n","  # randomly choose index in b \n","  # to ensure we have same number of type b and i\n","  # random.shuffle(cur_index)\n","  # cur_index_resized = cur_index[:min_num]\n","  if len(specs):\n","    specs = np.append(specs, specs_h5[cur_index], axis = 0)\n","  else:\n","    specs = specs_h5[cur_index]\n","\n","  if len(aux_input):\n","    aux_input = np.append(aux_input, aux_input_h5[cur_index], axis= 0)\n","  else:\n","    aux_input = aux_input_h5[cur_index]\n","  \n","  if len(sonotypes):\n","    sonotypes = np.append(sonotypes, np.repeat(i, len(cur_index)))\n","  else:\n","    sonotypes = np.repeat(i,len(cur_index))\n","\n","print(specs.shape)\n","print(aux_input.shape)\n","# print(sonotypes)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(499, 224, 224, 3)\n","(499, 4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ggxSJZ5fVJcu"},"source":["### thresholding/Normalization"]},{"cell_type":"code","metadata":{"id":"Kih_mjQhnFBX"},"source":["#  use threshold to get rid of small data\n","for i in range(len(specs)):\n","  threshold = np.percentile(specs[i], 80)\n","  specs[i] = np.where(specs[i] < threshold, 0, 255)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lw6aRCkRZMOj"},"source":["# normalizationã€ the data\n","for i in range(len(specs)):\n","  cur_spec = specs_keep[i]\n","  s_min = np.amin(cur_spec)\n","  s_max = np.amax(cur_spec)\n","  # specs[i] = (cur_spec - s_min)/(s_max - s_min) * 255\n","  specs[i] =(cur_spec - s_min)/(s_max - s_min) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n0nBVUOeU8UZ"},"source":["### randomly delete one data"]},{"cell_type":"code","metadata":{"id":"oAvKOIqJkquc","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1594001227776,"user_tz":300,"elapsed":1345,"user":{"displayName":"YUREN SUN","photoUrl":"","userId":"11675060901325192603"}},"outputId":"f7adbe08-a8eb-4e6d-8545-7917f15b5844"},"source":["# randomly delete one less data from current test data\n","num_sono = 3\n","cur_specs = np.copy(specs)\n","cur_aux = np.copy(aux_input)\n","cur_sonotypes = np.copy(sonotypes)\n","cur_selections = np.copy(selections)\n","\n","data_per_sono = int(cur_specs.shape[0] / num_sono)\n","for i in range(num_sono):\n","  index = random.randint(0, data_per_sono - 1)  + data_per_sono * i - i\n","  print(index)\n","  cur_specs = np.delete(cur_specs, index, 0)\n","  cur_aux = np.delete(cur_aux,index,0)\n","  cur_sonotypes = np.delete(cur_sonotypes,index,0)\n","  cur_selections = np.delete(cur_selections,index,0)\n","\n","print(data_per_sono)\n","print(cur_specs.shape)\n","print(cur_aux.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\n","12\n","22\n","12\n","(33, 224, 224, 3)\n","(33, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gpTYNZSvrJ7H"},"source":["specs = cur_specs\n","aux_input = cur_aux\n","sonotypes = cur_sonotypes\n","selections = cur_selections"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B_bnGCWaIe7K"},"source":["## Seperate test and train\n","Problem here: Althuogh sklearn train_test_split split randomly, in our case when we have the small dataset, chances are that the test/validation for differenct sonotypes differ a lot.\n","fixed in the later version of classification_with_augmentation."]},{"cell_type":"code","metadata":{"id":"GtNkroNccu7h","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595031029773,"user_tz":300,"elapsed":343,"user":{"displayName":"YUREN SUN","photoUrl":"","userId":"11675060901325192603"}},"outputId":"2323f223-7742-4d8e-b0a8-f3e38aa361de"},"source":["# seperate the data into training, validation, and testing\n","# 0.8 for train, 0.1 for val and test\n","x_train, x_test_val, y_train, y_test_val, aux_train, aux_test_val, sel_train, sel_test_val = train_test_split(specs, sonotypes, aux_input, selections, test_size=0.2)\n","\n","x_val, x_test, y_val, y_test, aux_val, aux_test, sel_val, sel_test = train_test_split(x_test_val, y_test_val, aux_test_val, sel_test_val, test_size=0.5)\n","\n","x_test = [x_test, aux_test]\n","cat_y_test = to_categorical(pd.factorize(y_test)[0],num_classes= len(typeUsed))\n","\n","# # test: work without aux input\n","# aux_train = np.zeros((len(x_train), 4))\n","# aux_test = np.zeros((len(x_test),4))\n","\n","print(len(x_train))\n","print(len(x_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["245\n","28\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L_mIU5MhrOjE"},"source":["# Test the model with MINST dataset"]},{"cell_type":"code","metadata":{"id":"P_3vZZb0rSwz","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593966203252,"user_tz":300,"elapsed":3536,"user":{"displayName":"YUREN SUN","photoUrl":"","userId":"11675060901325192603"}},"outputId":"3ede7fc2-f576-4f38-d743-df35ef86f937"},"source":["from keras.datasets.mnist import load_data\n","import cv2\n","\n","types = [1,5,4,3,2]\n","num = [72,72,72,72,72]\n","# load the data - it returns 2 tuples of digits & labels - one for\n","# the train set & the other for the test set\n","(train_digits, train_labels), (test_digits, test_labels) = load_data()\n","\n","train_num = np.multiply(num, 0.9)\n","test_num = np.multiply(num, 0.1)\n","\n","# re-shape the images data\n","train_data = []\n","test_data = []\n","train_y = []\n","test_y = []\n","for j in range(len(types)):\n","  cur_train = []\n","  for i in range(len(train_digits)):\n","    if train_labels[i] == types[j]:\n","      spec_resized=cv2.resize(train_digits[i],(224,224))\n","      spec_resized=cv2.cvtColor(spec_resized.astype('float32'), cv2.COLOR_BGR2RGB) #cv2 does not accept float64 \n","      spec_resized = np.flip(spec_resized,0)\n","      cur_train.append(spec_resized)\n","      train_y.append(train_labels[i])\n","    if len(cur_train) >= train_num[j]:\n","      if len(train_data):\n","        train_data = np.append(train_data, cur_train, axis = 0)\n","      else:\n","        train_data = cur_train\n","      break\n","  \n","\n","  cur_test = []\n","  for i in range(len(test_digits)):\n","    if test_labels[i] == types[j]:\n","      spec_resized=cv2.resize(test_digits[i],(224,224))\n","      spec_resized=cv2.cvtColor(spec_resized.astype('float32'), cv2.COLOR_BGR2RGB) #cv2 does not accept float64 \n","      spec_resized = np.flip(spec_resized,0)\n","      cur_test.append(spec_resized)\n","      test_y.append(test_labels[i])\n","    if len(cur_test) >= test_num[j]:\n","      if len(test_data):\n","        test_data = np.append(test_data, cur_test, axis = 0)\n","      else:\n","        test_data = cur_test\n","      break\n","\n","train_digits = []\n","train_labels = []\n","test_digits = []\n","test_labels = []"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dBT45tQQ3ozA","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"299eeec1-d82e-4a0f-e922-75d41da6833e"},"source":["len(train_data), len(test_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(325, 40)"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"nTMfnNCywvxf","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"31f1d1c1-311e-4b02-8a5b-33c148cc10b7"},"source":["from keras.utils import to_categorical\n","\n","cat_y_train = to_categorical(train_y[:1600])\n","cat_y_test = to_categorical(test_y[:200])\n","aux_train = np.zeros((len(train_data), 4))\n","aux_test = np.zeros((len(test_data),4))\n","x_train = train_data\n","x_test = test_data\n","\n","len(aux_train), len(aux_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(325, 40)"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"XZ-CAbI2YWEO"},"source":["# To Categorical"]},{"cell_type":"code","metadata":{"id":"FR0rlYOuYYEH","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595031031695,"user_tz":300,"elapsed":393,"user":{"displayName":"YUREN SUN","photoUrl":"","userId":"11675060901325192603"}},"outputId":"9867c6a9-babe-4c77-a45c-78fcb967555a"},"source":["# Convert labels to a categorical array\n","\n","num_c = max(len(np.unique(y_val)), len(np.unique(y_train)))\n","print(num_c)\n","cat_y_val = to_categorical(pd.factorize(y_val)[0],num_classes= num_c)\n","cat_y_train = to_categorical(pd.factorize(y_train)[0], num_classes= num_c)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PKSSG95w3er8"},"source":["# Model\n"]},{"cell_type":"code","metadata":{"id":"iRpZTBJ_V7dj"},"source":["config = dict(\n","    dropout = 0.55,\n","    hidden = 1024,\n","    learn_rate = 0.00001,\n","    epochs = 30,\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FxcBzs45VjPy"},"source":["def build_finetune_model(base_model, dropouts, fc_layers, num_classes):\n","    for layer in base_model.layers:\n","       layer.trainable = False\n","\n","    x = base_model.output\n","    x = Flatten()(x)\n","\n","    # add input layer\n","    auxiliary_input = Input(shape=(4,), name='aux_input')\n","    x = concatenate([x, auxiliary_input])\n","\n","    for fc, drop in zip(fc_layers, dropouts):\n","        x = Dense(fc, activation='relu')(x) \n","        x = Dropout(drop)(x)\n","\n","    predictions = Dense(num_classes, activation='softmax')(x)\n","\n","    finetune_model = Model(inputs=[base_model.input,auxiliary_input], outputs=predictions)\n","\n","    return finetune_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qzE1vnTqu5xv","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1602036310197,"user_tz":300,"elapsed":1075,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}},"outputId":"e2906fb5-c88e-4d99-f585-7fd460a42c3a"},"source":["model = None\n","keras.backend.clear_session()\n","model = VGG19(weights='imagenet', include_top=False, input_shape=(224,224,3))\n","model = build_finetune_model(model, \n","                             [config[\"dropout\"], config[\"dropout\"]], \n","                             [config[\"hidden\"], config[\"hidden\"]], \n","                             cat_y_train.shape[1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Large dropout rate: 0.55 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n","WARNING:tensorflow:Large dropout rate: 0.55 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x537wMr2SygI"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NSQxpgYTZsjm"},"source":["plot_model(model,to_file = 'My Drive/Stethoscope/model.png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h6nWIzk2Vl5-"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"Pw7naFs03eDb"},"source":["filepath = 'My Drive/Stethoscope/model_group.hdf5'\n","\n","earlystop = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n","checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n","\n","opt = Adam(lr=config[\"learn_rate\"])\n","model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","# history = model.fit(x=[x_train, aux_train], y=cat_y_train, validation_data=([x_val, aux_val], cat_y_val), epochs=50, verbose = 2)\n","history = model.fit(x=[x_train, aux_train], y=cat_y_train, validation_data=([x_val, aux_val], cat_y_val), epochs=config[\"epochs\"], callbacks=[earlystop, checkpoint])"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load the model with best loss\n","model = None\n","keras.backend.clear_session()\n","model = load_model (filepath)\n","# evaluation\n","results = model.evaluate( x= x_test, y=cat_y_test)\n","print(\"best loss test loss, test acc:\", results)"]}]}