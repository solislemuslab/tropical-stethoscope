{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sample_classification_with_augmentation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPB2maL9K1aSAUfxnzIlNZz"},"kernelspec":{"name":"python373jvsc74a57bd07588a3fb0c9403ab7ab30786f70afa789223f5fe30680232a90046c307ab79c7","display_name":"Python 3.7.3 64-bit (conda)"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"E7xnoDEz8R-Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622844370186,"user_tz":300,"elapsed":6582,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}},"outputId":"55cd32ff-4488-45f8-a09d-aa008db1103a"},"source":["# If run this file on google colab:\n","# Please use GPU for colab as CPU runtime would make the trainig process extremely slow\n","# It can be found in Notebook settings or Runtime > Change runtime type\n","# select GPU as Hardware accelerator.\n","# There are a limit for using GPU on google colab for free in a period of time\n","# but the quota is enough just to tryout this notebook\n","\n","%tensorflow_version 1.x\n","import pandas as pd\n","import h5py\n","import random\n","import numpy as np\n","import keras\n","from keras.optimizers import Adam\n","from keras.applications import VGG19\n","from keras.utils import to_categorical, plot_model\n","# from tensorflow.keras.callbacks import TensorBoard\n","from keras.models import Model, Sequential, load_model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.layers import Dense, Activation, Flatten, Dropout, Input, concatenate\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","import cv2\n","import tensorflow as tf"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"3NeB0pNE8cAe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622844394336,"user_tz":300,"elapsed":24158,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}},"outputId":"7006c719-a22f-46c6-bd43-7e6da27c37bc"},"source":["# if using google colab, run the following lines\n","from google.colab import drive\n","import os\n","\n","# give colab the right to access your file\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ghR6BTk93CI1"},"source":["# Read the dataset"]},{"cell_type":"code","metadata":{"id":"-LTQXpE4zOfG","executionInfo":{"status":"ok","timestamp":1622844413738,"user_tz":300,"elapsed":3997,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}}},"source":["# the data are stored in hdf5 format\n","# formated in five array for specs, sonotypes, \n","# start and end time times, min and max frequencies, and groups\n","\n","f =  h5py.File('My Drive/CNN-example/samples.hdf5', \"r\")\n","\n","specs_h5 = np.array(f[\"specs\"]).astype(\"float32\") # each element is a array of 224 * 224 * 3 floats\n","sonotypes_h5 = np.array(f[\"sonotypes\"]).astype(\"float32\") # each element is an floats\n","times_h5 = np.array(f[\"times\"]).astype(\"float32\") # each element is array of two floats\n","freqs_h5 = np.array(f[\"freqs\"]).astype(\"float32\") # each element is array of two floats\n","groups_h5 = np.array(f[\"groups\"]) # each element is a string\n","\n","f.close()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"T-1m0t2Y8m2w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622844429655,"user_tz":300,"elapsed":142,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}},"outputId":"2849bd94-b126-473d-9ba8-d906d41a154a"},"source":["# append x_times an x_freqs to be auxiliary_input\n","aux_input_h5 = np.append(times_h5,freqs_h5, axis = 1)\n","\n","# print the length and one of the sample\n","print(\"Number of samples:\",aux_input_h5.shape[0])\n","print(\"The auxiliary input of one sample:\",aux_input_h5[10])"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Number of samples: 808\n","The auxiliary input of one sample: [21411.273 21412.98  21411.273 21412.98 ]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LrILaIAW8nZp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622844431615,"user_tz":300,"elapsed":128,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}},"outputId":"21be614f-9f5c-41e0-c9b7-db3e7255475f"},"source":["# create the dictionary for sonotypes and groups\n","sono2group = dict(zip(sonotypes_h5,groups_h5))\n","\n","# sonotype and frequency in decending order\n","s_unique, s_freq = np.unique(sonotypes_h5,return_counts=True)\n","s_freq_order = np.argsort(s_freq)[::-1]\n","s_freq_desc = s_freq[s_freq_order]\n","\n","# print some stat\n","print(\"Number of sonotypes:\", len(s_unique))\n","print(\"Sonotype in decending order of sample size:\", s_unique[s_freq_order][:20])\n","print(\"Sample size:\",s_freq_desc[:20])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Number of sonotypes: 13\n","Sonotype in decending order of sample size: [ 52. 138. 283. 463.  86. 175.  -3.  -4.  -7.  -6.  -2.  -8.  -5.]\n","Sample size: [231 153 109  91  62  49  35  28  24  13   6   4   3]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3a9UV4WZ1ILT","executionInfo":{"status":"ok","timestamp":1622844444595,"user_tz":300,"elapsed":121,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}},"outputId":"08ed9569-ecd1-4307-bc9d-2d178665f2a2"},"source":["# pick sonotypes\n","\n","numUsed =6 # number of sonotype to use, at most 6 (will pick 6 if > 6)\n","groupUsed = b'b' # the group to use, the sample dataset only contain birds\n","\n","# randomly pick numUsed number of sonotype\n","# positive sonotype are for bird sounds,\n","# negative sonotypes are for noises used in data agumentation\n","typeUsed = [x for x in s_unique if x > 0] \n","random.shuffle(typeUsed)\n","typeUsed = typeUsed[:numUsed]\n","\n","print(\"Sonotype used:\", typeUsed)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Sonotype used: [175.0, 283.0, 463.0, 86.0, 138.0, 52.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zmet4PKD8ow8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622844446029,"user_tz":300,"elapsed":326,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}},"outputId":"96859540-b9ba-42c4-8cb2-0c2330cee7e5"},"source":["# get the samples, balanced samples\n","\n","# sample size to use for each sonotype\n","num_pick = 49\n","\n","# train\n","specs = []\n","aux_input = []\n","sonotypes = []\n","\n","# test and validation\n","spec_test = []\n","aux_test = []\n","y_test = []\n","spec_val = []\n","aux_val = []\n","y_val = []\n","\n","for i in range(len(typeUsed)):\n","  # get index of the current type of spec\n","  cur_index = np.argwhere(sonotypes_h5 == typeUsed[i]).flatten()\n","  \n","  # categorize into train, validate, and test in 8:1:1 ratio\n","  random.shuffle(cur_index)\n","  cur_index_resized = cur_index[:int(num_pick * 0.8)]\n","  test_index = cur_index[int(num_pick * 0.8): int(num_pick * 0.9)]\n","  val_index = cur_index[int(num_pick * 0.9): num_pick]\n","\n","  # put into each list\n","  if len(specs):\n","    # not null\n","    specs = np.append(specs, specs_h5[cur_index_resized], axis=0)\n","    aux_input = np.append(\n","        aux_input, aux_input_h5[cur_index_resized], axis=0)\n","    # sonotypes in training are start from 0\n","    sonotypes = np.append(sonotypes, np.repeat(i, int(num_pick * 0.8)))\n","    spec_test = np.append(spec_test, specs_h5[test_index], axis=0)\n","    aux_test = np.append(aux_test, aux_input_h5[test_index], axis=0)\n","    spec_val = np.append(spec_val, specs_h5[val_index], axis=0)\n","    aux_val = np.append(aux_val, aux_input_h5[val_index], axis=0)\n","    y_test = np.append(y_test, np.repeat(i, len(test_index)))\n","    y_val = np.append(y_val, np.repeat(i, len(val_index)))\n","  else:\n","    specs = specs_h5[cur_index_resized]\n","    aux_input = aux_input_h5[cur_index_resized]   \n","    # sonotypes in training are start from 0  \n","    sonotypes = np.repeat(i, int(num_pick * 0.8))\n","    spec_test = specs_h5[test_index]\n","    aux_test = aux_input_h5[test_index]\n","    spec_val = specs_h5[val_index]\n","    aux_val = aux_input_h5[val_index]\n","    y_test = np.repeat(i, len(test_index))\n","    y_val = np.repeat(i, len(val_index))\n","\n","# format the input for test and validation into the required format\n","x_test = [spec_test, aux_test]\n","x_val = [spec_val, aux_val]\n","\n","# categorical the output for test and validation into the required format\n","cat_y_test = to_categorical(pd.factorize(y_test)[0], num_classes=len(typeUsed))\n","cat_y_val = to_categorical(pd.factorize(y_val)[0], num_classes=len(typeUsed))\n","\n","# just make a copy of specs for training\n","specs_keep = np.copy(specs)\n","\n","# just print some stats\n","print(\"Number of sample for tests or validation:\", len(y_test))\n","print(\"Number of samples for training:\", specs.shape[0])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Number of sample for tests or validation: 30\n","Number of samples for training: 234\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_VSf-dXWsHbV"},"source":["# Normalization"]},{"cell_type":"code","metadata":{"id":"8vdu4viHsDyH","executionInfo":{"status":"ok","timestamp":1622844482664,"user_tz":300,"elapsed":152,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}}},"source":["def normalize(specs):\n","  '''\n","  Linear normalization of the data\n","  @param: specs is the list of spcetrograms to normalize\n","  @return: the normalized spectrograms\n","  '''\n","\n","  return_specs = []\n","  for i in range(len(specs)):\n","    # make a copy to ensure not changing the original spectrogram\n","    cur_spec = np.copy(specs[i])\n","    s_min = np.amin(cur_spec)\n","    s_max = np.amax(cur_spec)\n","    return_specs.append((cur_spec - s_min)/(s_max - s_min) * 255)\n","\n","  return return_specs"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Divm4kJd-ufK"},"source":["# Augmentation Methods\n","\n","For each function, make a copy of the original spectrogram\n","to ensure that we do not change the original one\n","\n","Return all the augmented spectrograms in lists for consistency"]},{"cell_type":"code","metadata":{"id":"jeHY3yeO-0FX","executionInfo":{"status":"ok","timestamp":1622844487388,"user_tz":300,"elapsed":140,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}}},"source":["def time_chop(spec, rand_start):\n","  '''\n","  chop the spectrogram on x axis (time) from the right\n","  @param: spec, the spectrogram to chop\n","  @param: rand_start: the randomed index to start chopping\n","  @return: the list of augmented spectrograms\n","  '''\n","\n","  time_chopped_spec = np.copy(spec)\n","  time_chopped_spec[:,224 - rand_start:,:] = 0\n","\n","  return [time_chopped_spec]\n","\n","def freq_chop(spec, rand_start):\n","  '''\n","  chop the spectrogram on y axis (frequency) from the top\n","  @param: spec, the spectrogram to chop\n","  @param: rand_start: the randomed index to start chopping\n","  @return: the list of augmented spectrograms\n","  '''\n","\n","  freq_chopped_spec = np.copy(spec)\n","  freq_chopped_spec[0:rand_start,:,:] = 0\n","\n","  return [freq_chopped_spec]\n","\n","def four_chop(spec, rand_start):\n","  '''\n","  chop the spectrogram on four sides\n","  @param: spec, the spectrogram to chop\n","  @param: rand_start: the randomed index to start chopping\n","  @return: the list of augmented spectrograms\n","  '''\n","\n","  four_chopped_spec = np.copy(spec)\n","  four_chopped_spec[0 : rand_start,:,:] = 0  # top\n","  four_chopped_spec[:,224 - rand_start:,:] = 0  # right\n","  four_chopped_spec[224 - rand_start:,:,:] = 0  # bottom\n","  four_chopped_spec[:, 0 : rand_start ,:] = 0  # left\n","\n","  return [four_chopped_spec]"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"2s1Hlo6q9-BC","executionInfo":{"status":"ok","timestamp":1622844487902,"user_tz":300,"elapsed":135,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}}},"source":["def add_noises(spec):\n","  '''\n","  add noise to the spectrogram with 1/3 ratio\n","  @param: spec, the spectrogram to chop\n","  @return: the list of augmented spectrograms\n","  '''\n","  # add noise from light rian -2, rain -3, heavy rain -4, thunder -5, aircraft -6, chainsaw -7, and car/truck -8\n","  return_specs = []\n","  noise_sonos = [-2, -3, -4, -5,-6,-7,-8]\n","\n","  for i in range(len(noise_sonos)):\n","    noises_index = np.argwhere(sonotypes_h5 == noise_sonos[i]).flatten()\n","    noises = specs_h5[noises_index]\n","    # randomly pick a noise sample\n","    index = random.randint(0, len(noises) - 1)\n","    # normalize sound and noise, add them together with 1/3 ratio\n","    noise = normalize(np.array(noises[index]) / 3)\n","    return_specs.append(np.add(normalize([np.copy(spec)])[0], noise))\n","\n","  return return_specs"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"H3lLYDZU_VAJ","executionInfo":{"status":"ok","timestamp":1622844488348,"user_tz":300,"elapsed":129,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}}},"source":["def translate(spec, roll_start):\n","  '''\n","  roll the spectrogram up and down\n","  @param: spec, the spectrogram to chop\n","  @param: roll_start, the index to start rolling\n","  @return: the list of augmented spectrograms\n","  '''\n","\n","  return_specs = []\n","  return_specs.append(np.roll(spec, -roll_start, axis = 0))\n","  return_specs.append(np.roll(spec, roll_start, axis = 0))\n","\n","  return return_specs"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMRDhokq_Wg9","executionInfo":{"status":"ok","timestamp":1622844488758,"user_tz":300,"elapsed":5,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}}},"source":["def widen(spec, widen_index):\n","  '''\n","  widen the spectrogram\n","  @param: spec, the spectrogram to chop\n","  @param: widen_index, the index to decide the start and end of\n","          the spectrogram to widen\n","  @return: the list of augmented spectrograms\n","  '''\n","  return_specs = []\n","  widen_time_spec=cv2.resize(spec.astype('float32'),(224 + widen_index,224))\n","  widen_freq_spec=cv2.resize(spec.astype('float32'),(224,224 + widen_index))\n","\n","  return_specs.append(widen_time_spec[:,widen_index // 2: -widen_index // 2,:])\n","  return_specs.append(widen_freq_spec[widen_index // 2: -widen_index // 2,:,:])\n","\n","  return return_specs\n","\n","def squeeze(spec, squeeze_index):\n","  '''\n","  squeeze the spectrogram\n","  @param: spec, the spectrogram to chop\n","  @param: widen_index, the index to decide the start and end of\n","          the spectrogram to widen\n","  @return: the list of augmented spectrograms\n","  '''\n","  \n","  squeezed=cv2.resize(spec.astype('float32'),(224 - squeeze_index,224 - squeeze_index))\n","  squeeze_spec = np.zeros([224, 224, 3])\n","  squeeze_spec[squeeze_index//2 : - squeeze_index //2,squeeze_index//2 : - squeeze_index //2, :] = squeezed\n","\n","  return [squeeze_spec]"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"XfJQnb19kqGn","executionInfo":{"status":"ok","timestamp":1622844489337,"user_tz":300,"elapsed":159,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}}},"source":["def augment(specs, aux_input, sonotypes, aug_num, augment_range = 0.1):\n","  '''\n","  call all the augment methods on the spectrograms\n","\n","  @param: specs is the list of spectrograms to augment from\n","  @param: aux_input is the list of auxiliary input corresponds to the spectrograms\n","  @param: sonotypes is the list of sonnotypes corresponds to the spectrograms\n","  @param: aug_num is the number of sets of augmented spectrograms\n","          (returned number of samples will be 1 + 15*aug_num)\n","  @param: augment_range is the threshold used for augmentations, default to 0.1\n","  @return: augment_specs_func is the list of augmented spectrograms\n","  @return: augment_aux_func is the list of  auxiliary input corresponds to the spectrograms\n","  @return: augment_sono_func is the list of sonotypes input corresponds to the spectrograms\n","  '''\n","\n","  augment_specs_func = []\n","  augment_aux_func = []\n","  augment_sono_func = []\n","\n","  # print(len(aux_input))\n","  for i in range(len(specs)):\n","    # generate random index array for augmentation\n","    # in 5% to 10% of the size of the original spectrogram\n","    # 224 * 224 is the image size\n","    indices = np.arange(int(224 * augment_range / 3 * 2) , int(224 * augment_range))\n","    np.random.shuffle(indices)\n","    indices = indices[:aug_num]\n","    \n","    # augment each spec and add to list\n","    cur_spec = np.copy(specs[i])\n","    # add itself to the list\n","    if (len(augment_specs_func)):\n","      augment_specs_func = np.append(augment_specs_func, [cur_spec], axis = 0)\n","    else:\n","      augment_specs_func.append(cur_spec)\n","    # augment_specs_func.append(cur_spec)\n","\n","    for index in indices:\n","      # print(index)\n","      # chop\n","      augment_specs_func = np.append(augment_specs_func, time_chop( np.copy(cur_spec), index), axis = 0)\n","      augment_specs_func = np.append(augment_specs_func, freq_chop( np.copy(cur_spec), index), axis = 0)\n","      augment_specs_func = np.append(augment_specs_func, four_chop( np.copy(cur_spec), index), axis = 0)\n","\n","      # widen + squeeze\n","      augment_specs_func = np.append(augment_specs_func, squeeze( np.copy(cur_spec), index), axis = 0)\n","      augment_specs_func = np.append(augment_specs_func, widen( np.copy(cur_spec), index), axis = 0)\n","      \n","      # noise\n","      augment_specs_func = np.append(augment_specs_func, add_noises(np.copy(cur_spec)), axis = 0)\n","\n","      # translate\n","      augment_specs_func = np.append(augment_specs_func, translate(np.copy(cur_spec), index), axis = 0)\n","\n","    # total 1 + 15 * aug_num augmented, repeat the sono and aux\n","    if (len(augment_aux_func)):\n","      augment_aux_func = np.append(augment_aux_func, np.repeat([aux_input[i]], 1 + 15 * aug_num, axis = 0), axis= 0)\n","    else:\n","      augment_aux_func = np.repeat([aux_input[i]], 1 + 15 * aug_num, axis = 0)\n","\n","    augment_sono_func = np.append(augment_sono_func, np.repeat(sonotypes[i], 1 + 15 * aug_num), axis= 0)\n","\n","  return augment_specs_func, augment_aux_func, augment_sono_func"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cHlMGXpZBET4"},"source":["# Model and Training"]},{"cell_type":"code","metadata":{"id":"tmpT97hABD3x","executionInfo":{"status":"ok","timestamp":1622844492256,"user_tz":300,"elapsed":542,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}}},"source":["config = dict(\n","    dropout = 0.5,\n","    hidden = 1024,\n","    learn_rate = 0.00001,\n","    epochs = 30,\n","    )"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"89CFyhKcBHgW","executionInfo":{"status":"ok","timestamp":1622844492402,"user_tz":300,"elapsed":2,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}}},"source":["def build_finetune_model(base_model, dropouts, fc_layers, num_classes):\n","    '''\n","    finetune the model, freeze teh top layers,\n","    add dropouts, dense layers, \n","    another input layer for auxiliary input \n","    and concatenate it with the flatten layer\n","    '''\n","    \n","    # freeze the base layers\n","    for layer in base_model.layers:\n","       layer.trainable = False\n","\n","    # add flatten layer\n","    x = base_model.output\n","    x = Flatten()(x)\n","\n","    # add input layer for auxiliary input (time and frequency)\n","    auxiliary_input = Input(shape=(4,), name='aux_input')\n","    x = concatenate([x, auxiliary_input])\n","\n","    #  dense and dropout layer\n","    for fc, drop in zip(fc_layers, dropouts):\n","        x = Dense(fc, activation='relu')(x) \n","        x = Dropout(drop)(x)\n","\n","    # final dense layer for output\n","    predictions = Dense(num_classes, activation='softmax')(x)\n","\n","    finetune_model = Model(inputs=[base_model.input,auxiliary_input], outputs=predictions)\n","\n","    return finetune_model"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"6id17t85n_3p","executionInfo":{"status":"ok","timestamp":1622844496186,"user_tz":300,"elapsed":143,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}}},"source":["class TestCallback(keras.callbacks.Callback):\n","    '''\n","    The class used to see the test result during training\n","    '''\n","    \n","    def __init__(self, test_data):\n","        self.test_data = test_data\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        x, y = self.test_data\n","        loss, acc = self.model.evaluate(x, y, verbose=0)\n","        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"-wPPQ60_rctY","executionInfo":{"status":"ok","timestamp":1622844497546,"user_tz":300,"elapsed":143,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}}},"source":["def gen(specs, aux_input, sonotypes):\n","  '''\n","  generator functiion for fit_generator\n","  augment the samples and yield the augmented samples to train the model\n","  \n","  As the data would be too large if we augment all the images before training, \n","  we need to use this function to augment the images during the training process\n","\n","  @param: specs, the list of spectrograms\n","  @param: aux_input, the list of auxiliary input corresponding to the spectrograms\n","  @param: sonotypes, the list of sonotypes corresponding to the spectrograms\n","  '''\n","\n","  # augment_specs, augment_aux, augment_sono =  augment(specs_seperated[i], aux_seperated[i], sono_seperated[i], 1)\n","  while 1:\n","    # shuffle data\n","    indices = np.arange(len(sonotypes))\n","    np.random.shuffle(indices)\n","    # augment 4 samples to get 64 samples per yield (suitable for training)\n","    step_len = 4\n","\n","    for i in range(len(specs) // step_len):\n","      step_min = i * step_len\n","      step_max = min( (i + 1) * step_len, len(specs) )\n","      \n","      augment_specs, augment_aux, augment_sono =  augment(specs[indices][step_min: step_max], aux_input[indices][step_min: step_max], sonotypes[indices][step_min: step_max], 1)\n","\n","      # normalize spectrograms and categorical outputs\n","      augment_specs_normal = normalize(augment_specs)\n","      cat_y_train = to_categorical(augment_sono, num_classes= len(typeUsed))\n","\n","      yield {'input_1': np.array([augment_specs_normal])[0], 'aux_input': np.array([augment_aux])[0]}, np.array([cat_y_train])[0]"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"9r8uQ7upBJBe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622844510121,"user_tz":300,"elapsed":10823,"user":{"displayName":"YUREN SUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij_u8dz_gjnTlGMTY6-5CE94S6R4yHlxYjbV0vTQ=s64","userId":"11675060901325192603"}},"outputId":"691f349b-eee7-42ca-80c4-f0f79031001c"},"source":["model = None\n","keras.backend.clear_session()\n","# get the pretrained model\n","model = VGG19(weights='imagenet', include_top=False, input_shape=(224,224,3))\n","# finetune to our case\n","model = build_finetune_model(model, \n","                             [config[\"dropout\"], config[\"dropout\"]], \n","                             [config[\"hidden\"], config[\"hidden\"]], \n","                             len(typeUsed))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n","80142336/80134624 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wvacKNbVBOTe"},"source":["# model stats, remove \"#\" in lines below to print\n","# model.summary()\n","# plot_model(model,to_file = 'My Drive/example/model.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1vd2PGgjfHl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d0992eb2-244c-4057-95cb-e7a47e82ec8d"},"source":["# we need to reset the model when starting a new training process, can be done with codes below \n","# (the same as in one of the above cells, just copy here for easier use)\n","\n","# model = None\n","# keras.backend.clear_session()\n","# # get the pretrained model\n","# model = VGG19(weights='imagenet', include_top=False, input_shape=(224,224,3))\n","# # finetune to our case\n","# model = build_finetune_model(model, \n","#                              [config[\"dropout\"], config[\"dropout\"]], \n","#                              [config[\"hidden\"], config[\"hidden\"]], \n","#                              len(typeUsed))\n","\n","# training\n","filepath_loss = 'My Drive/CNN-example/model_loss.hdf5'\n","\n","# remove model before\n","if os.path.exists(filepath_loss):\n","  os.remove(filepath_loss)\n","\n","# early stopping and checkpoint to save the model with the lowest validation\n","earlystop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n","checkpoint = ModelCheckpoint(filepath_loss, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n","\n","# optimization method for training\n","opt = Adam(lr=config[\"learn_rate\"])\n","\n","\n","model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# if we want to see the test result along the training, add \"TestCallback((x_test, cat_y_test))\" to callbacks\n","# i.e. callbacks=[checkpoint_loss, checkpoint_acc, TestCallback((x_test, cat_y_test))]\n","\n","# with augmentation\n","history = model.fit_generator(gen(specs, aux_input, sonotypes),\n","                    steps_per_epoch=len(specs) // 4, epochs = 300, validation_data = (x_val, cat_y_val), callbacks=[checkpoint, earlystop])\n","\n","# without augmentation\n","# cat_y_train = to_categorical(sonotypes, num_classes=len(typeUsed))\n","# history = model.fit(x=[specs, aux_input], y=cat_y_train, validation_data=(x_val, cat_y_val), epochs=300, verbose=2, callbacks=[checkpoint, earlystop])"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"p113jhUijjfI"},"source":["# Evaluate the model\n","\n","# current model\n","results = model.evaluate( x= x_test, y=cat_y_test)\n","print(\"cur test loss, test acc:\", results)\n","\n","# The model with best loss\n","# load the model\n","model = None\n","keras.backend.clear_session()\n","model = load_model (filepath_loss)\n","\n","# evaluate\n","results = model.evaluate( x= x_test, y=cat_y_test)\n","print(\"best loss test loss, test acc:\", results)"],"execution_count":null,"outputs":[]}]}